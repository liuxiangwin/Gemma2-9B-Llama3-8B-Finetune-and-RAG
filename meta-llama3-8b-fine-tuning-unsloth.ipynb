{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib64/python3.11/site-packages (23.2.1)\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/ef/7d/500c9ad20238fcfcb4cb9243eede163594d7020ce87bd9610c9e02771876/pip-24.3.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-24.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2024.11.7 requires protobuf<4.0.0, but you have protobuf 5.28.3 which is incompatible.\n",
      "kfp 2.9.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.28.3 which is incompatible.\n",
      "kfp-kubernetes 1.3.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.28.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.4.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.9.0 requires protobuf<5,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "kfp-kubernetes 1.3.0 requires protobuf<5,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.4.0 requires protobuf<5,>=4.21.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: xformers in /opt/app-root/lib64/python3.11/site-packages (0.0.28.post1)\n",
      "Requirement already satisfied: trl in /opt/app-root/lib64/python3.11/site-packages (0.12.1)\n",
      "Requirement already satisfied: peft in /opt/app-root/lib64/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib64/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: bitsandbytes in /opt/app-root/lib64/python3.11/site-packages (0.44.1)\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2024.11.7 requires protobuf<4.0.0, but you have protobuf 5.28.3 which is incompatible.\n",
      "kfp 2.9.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.28.3 which is incompatible.\n",
      "kfp-kubernetes 1.3.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.28.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.4.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git --quiet\n",
    "!pip install trl wandb --quiet\n",
    "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "# !pip install --no-deps \"xformers<0.0.26\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "!pip install  --upgrade --quiet \\\n",
    "  \"datasets>=2.21.0\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"pillow\" \\\n",
    "  \"hyperopt\" \\\n",
    "  \"optuna\" \\\n",
    "  \"protobuf>=4.21.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# # !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# !pip install \"unsloth[cu121-torch240] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "# if major_version >= 8:\n",
    "#     # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n",
    "#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "# else:\n",
    "#     # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n",
    "#     !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "# pass\n",
    "# !pip install triton transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install --pre -U xformers ##### this take some time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****Note** Restart the Kernal after package installation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from IPython.display import display_markdown\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",  \n",
    "]  #### loadin llama 3 model in 4 bit to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.9: Fast Llama patching. Transformers = 4.47.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 21.975 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref_config https://github.com/pytorch/torchtune/blob/main/recipes/configs/llama3/8B_qlora_single_device.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dataset import \n",
    "dataset_name = \"VMware/open-instruct\"\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must add EOS_TOKEN at response last line\n",
    "EOS_TOKEN = tokenizer.eos_token \n",
    "def mapping_response(sample):\n",
    "    sample['text'] = sample['alpaca_prompt']+\"\\n\"+sample['response']+EOS_TOKEN\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting only 2000 samples for testing\n",
    "dataset = dataset.select(range(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(mapping_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_inference(prmpt):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prmpt\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
    "    return tokenizer.batch_decode(outputs)[0].split(\"### Response:\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate the lyrics to a song that begins like this \"Hey hey let's go...\"\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "prompt = dataset['alpaca_prompt'][90]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "```python\n",
       "lyrics = \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's go\"\n",
       "lyrics += \"Hey hey let's"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"result\")\n",
    "display_markdown(prompt_inference(prmpt=prompt),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the above result is not clear ,without fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fa1fd5ed7b493391f709c4dd9e02df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", ### taking text column from the dataset\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 500,\n",
    "        max_steps = 800,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",num_train_epochs =1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 2 | Total steps = 800\n",
      " \"-____-\"     Number of trainable parameters = 20,971,520\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliuxiangwin\u001b[0m (\u001b[33mliuxiangwin-free\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/app-root/src/Gemma2-9B-Llama3-8B-Finetune-and-RAG/wandb/run-20241126_065202-p4fvsham</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuxiangwin-free/huggingface/runs/p4fvsham' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/liuxiangwin-free/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuxiangwin-free/huggingface' target=\"_blank\">https://wandb.ai/liuxiangwin-free/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuxiangwin-free/huggingface/runs/p4fvsham' target=\"_blank\">https://wandb.ai/liuxiangwin-free/huggingface/runs/p4fvsham</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 10:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.191500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=1.199762134552002, metrics={'train_runtime': 650.0535, 'train_samples_per_second': 2.461, 'train_steps_per_second': 1.231, 'total_flos': 2.053034377592832e+16, 'train_loss': 1.199762134552002, 'epoch': 0.8})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "I want to create a super-powered character with their own perks and background, and even a backstory. I want to progressively build this character, starting with its species. I want you to give me a list of 6 species that this character could be (e.g. human, demon, android, etc.) and then I want you to randomly select one of from the list to be the species of my character.\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "prompt = dataset['alpaca_prompt'][92]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "Here is a list of 6 species that your character could be:\n",
       "\n",
       "1. Human\n",
       "2. Demon\n",
       "3. Android\n",
       "4. Vampire\n",
       "5. Elf\n",
       "6. Dragon\n",
       "\n",
       "I randomly selected \"Human\" as the species of your character. Your character is a human, but with some unique powers and abilities. They have the ability to control fire, which allows them to manipulate flames and use them as a weapon. They also have the ability to fly, which gives them a unique advantage in combat. Your character's backstory is that they were born with these abilities and have been training to use them since a young age. They have been using their powers to protect the world from evil forces and to fight for justice. Your character is a hero, and they will use their powers to make the world a better place.<|end_of_text|>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"result\")\n",
    "display_markdown(prompt_inference(prmpt=prompt),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
