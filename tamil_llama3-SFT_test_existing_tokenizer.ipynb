{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b43de97-6a76-463e-a10d-2071fd563f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf scikit-learn scipy --progress-bar off --quiet\n",
    "!pip install wandb --progress-bar off --quiet\n",
    "!pip install -q accelerate  bitsandbytes  trl==0.4.7 --progress-bar off --quiet\n",
    "!pip install  transformers[sentencepiece] --progress-bar off --quiet\n",
    "!pip install sentencepiece --progress-bar off --quiet\n",
    "!pip install tensorboardX --progress-bar off --quiet\n",
    "!pip install -U accelerate==0.29.3 peft==0.10.0 bitsandbytes>=0.41.3 transformers==4.40.0 trl==0.8.5 --progress-bar off --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4526cde9-8b9b-4917-bb50-d1dd284de4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /opt/app-root/src/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9985edf-667a-4326-81c1-f5526bf34375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"Hemanth-thunder/tamil-open-instruct-v1\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-3-8b-tamil-open-instruct-v1\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "# max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b93ec1-47a0-45e9-926b-0bcae1c0fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adcefe75-8f08-4bad-8a9d-14a2a8a26507",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = dataset.shuffle().select(range(150000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dd0c0e5-d31b-469e-b6b4-1da7358d991b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input', 'text'],\n",
       "    num_rows: 150000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf647af-06a1-4934-a27e-df9b3547389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, வழங்கப்பட்ட வழிகாட்டுதல்களைப் பின்பற்றி, தேவையான தகவலை உள்ளிடவும்.\n",
      "\n",
      "### Instruction:\n",
      "பின்வரும் பழங்களை அமிலம் அல்லது இனிப்பு என வகைப்படுத்தவும் \n",
      "\n",
      "### Input:\n",
      "எலுமிச்சை, ஆப்பிள், பீச்\n",
      "\n",
      "### Response:\n",
      "எலுமிச்சை: அமில ஆப்பிள்: இனிப்பு பீச்: இனிப்பு\n"
     ]
    }
   ],
   "source": [
    "print(sample_dataset['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfbb931-68dc-42fc-bcfc-e2e0fd9277fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df659995ff824944aaa1f9c119b3447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################\n",
    "###########################################################################\n",
    "#It you want sucessfully run this code you need set device_map=\"auto\"\n",
    "# meanwhile set the 'CUDA_VISIBLE_DEVICES' only one device\n",
    "###########################################################################\n",
    "###########################################################################\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afafb0bd-29bc-4800-ae50-1bf7bcbeff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da6f461-1cec-4500-9022-11ed50875f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b999ca94f554057b34507527c6e39f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Must add EOS_TOKEN at response last line\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "EOS_TOKEN = tokenizer.eos_token \n",
    "def prompt_eos(sample):\n",
    "    sample['text'] = sample['text']+EOS_TOKEN\n",
    "    return sample\n",
    "sample_dataset = sample_dataset.map(prompt_eos)\n",
    "\n",
    "# # Must add EOS_TOKEN at response last line\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# EOS_TOKEN = tokenizer.eos_token \n",
    "# def prompt_eos(sample):\n",
    "#     sample['text'] = sample['text']+EOS_TOKEN\n",
    "#     return sample\n",
    "# dataset = dataset.map(prompt_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c197b380-2c70-4c6a-a4a9-97fbe62d7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model=model, peft_config=peft_config)\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps = 500,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=500,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d21207-8de4-4b98-bf1b-66f2fa43456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12168034-3c16-40f9-9144-29cb0890ab50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 09:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.474800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.404500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.402800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.4733975315093994, metrics={'train_runtime': 570.0362, 'train_samples_per_second': 1.754, 'train_steps_per_second': 0.877, 'total_flos': 2.848043677586227e+16, 'train_loss': 0.4733975315093994, 'epoch': 0.0020250580685401156})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1fa166-3fd6-4f8e-8da2-23c210c5e343",
   "metadata": {},
   "outputs": [],
   "source": [
    " def generate_response(prompt, model):\n",
    "    encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "    \n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=512, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "    \n",
    "    return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "098c5a4c-5de1-4062-8ec2-dad8e5784613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' வேலை நேர்காணலுக்கான சிறந்த குறிப்புகள் என்ன.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset['instruction'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d233579-fefe-4234-ba46-a5c165bdc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "# with Input\n",
    "prompt_template_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, வழங்கப்பட்ட வழிகாட்டுதல்களைப் பின்பற்றி, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb03cea4-c1a2-4842-8f74-f70749687936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_prompt(sample):\n",
    "#     if 'nan' not in sample['input']:\n",
    "#       prompt = prompt_template_input.format(sample['instruction'][-1].strip(),sample['input'])\n",
    "#     else:\n",
    "#      prompt = prompt_template_wo_input.format(sample['instruction'][-1].strip())\n",
    "#     return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4abc647-ecfe-4189-a036-7bd0dfeb49b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> வணக்கம். நான் அந்த கணக்கை அறிந்துள்ளேன். கடவுள் உங்களை நன்றாக இருக்க விரும்புகிறார். நீங்கள் என்ன பொருத்தம் வேண்டும்? வேலையை முடிக்கவும். வணக்கம். நான் அந்த கணக்கை அறிந்துள்ளேன். கடவுள் உங்களை நன்றாக இருக்க விரும்புகிறார். நீங்கள் என்ன பொருத்தம் வேண்டும்? வேலையை முடிக்கவும். வணக்கம். நான் அந்த கணக்கை அறிந்துள்ளேன். கடவுள் உங்களை நன்றாக இருக்க விரும்புகிறார். நீங்கள் என்ன பொருத்த�'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "வணக்கம்\n",
    "\n",
    "### Response:\"\"\"\n",
    "generate_response(prompt_template_wo_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e670ec48-466f-4f23-ba9b-0139e20d1fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|end_of_text|>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "இந்தியாவின் தலைநகரம் என்ன?\n",
    "\n",
    "### Response:\"\"\"\n",
    "generate_response(prompt_template_wo_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8fdb161-00bc-4c66-81d6-576e21f17fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> நீ என்ன வகையான மனிதன்? நான் ஒரு மனிதன். நீ எந்த வகையான மனிதன்? நான் ஒரு பெண். நீ எந்த வகையான மனிதன்? நான் ஒரு கடலோர மனிதன். நீ எந்த வகையான மனிதன்? நான் ஒரு தீவிர விடுதி கடலோர மனிதன். நீ எந்த வகையான மனிதன்? நான் ஒரு தீவிர விடுதி கடலோர மனிதன். நீ எந்த வகையான மனிதன்? நான் ஒரு தீவிர விடுதி கடலோர மனிதன். நீ எந்த வகையான மனிதன்? நான் ஒரு தீவிர விடுதி கடலோர மனிதன். நீ எந்த வகையான'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "யார் நீ\n",
    "\n",
    "### Response:\"\"\"\n",
    "generate_response(prompt_template_wo_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aec4efd4-e44f-47b9-8ba7-6d3610b6bcef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> எ-காமர்ஸ் இணையதளத்தை உருவாக்குவதற்கு செலவு சுமார் $10,000 சதுர அடி. செலவு கூடும் அளவில் உள்ள விவரங்கள் மற்றும் பொருத்தமான செலவைக் கணக்கிடவும். உங்கள் செலவு அளவு அதிகரிக்க வேண்டும் என்றால், பொதுவாக பல மாற்றுத்திறன் அளவு மற்றும் வருவாய் கூடுதல் உள்ளது. எனவே அது கட்டுமானத்திற்கு செலவைக் கணக்கிடுவதற்கு மற்றும் உங்கள் இ-காமர்ஸ் வணிகத்தை வளர்த்து வைக்க முடியும். எனவே பொதுவாக செ�'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "இ-காமர்ஸ் இணையதளத்தை உருவாக்குவதற்கான செலவை மதிப்பிடவும்.\n",
    "\n",
    "### Response:\"\"\"\n",
    "generate_response(prompt_template_wo_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d485e3a8-0596-4c9c-a8b9-c12004f9ea55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> வரலாற்றாசிரியர் என்று கற்பனை செய்யப்பட்டால், அச்சகத்தின் முக்கியத்துவம் மற்றும் அதன் தாக்கம் பற்றிய வரலாறு எழுதுவேன். பாராடக்ஸ் வரலாறு எழுதுவது என்றால், அது சமூகத்தில் தாக்கம் செலுத்தும் என்று நான் நம்புகிறேன். பாராடக்ஸ் வரலாறு எழுதுவது என்றால், அது சமூகத்தில் தாக்கம் செலுத்தும் என்று நான் நம்புகிறேன். பாராடக்ஸ் வரலாறு எழுதுவது என்றால், அது சமூகத்தில் தாக்கம் செலுத்தும்'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "நீங்கள் ஒரு வரலாற்றாசிரியர் என்று கற்பனை செய்து பாருங்கள். மறுமலர்ச்சிக் காலத்தில் அச்சகத்தின் முக்கியத்துவத்தையும் சமூகத்தில் அதன் தாக்கத்தையும் விளக்கவும்.\n",
    "\n",
    "### Response:\"\"\"\n",
    "generate_response(prompt_template_wo_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "676e72ed-d85e-4419-98c8-0e86078a6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"llama3-fine-tuning/tamil-llama3-8B-test-open-instruct-v1-peft\" #Name of the model you will be pushing to huggingface model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9dde8d6-ef2b-4410-a315-4454a6bef950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcf78d6b-becd-4974-bcd7-bbf1cb3f32ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b2138de07c4ab59e2364b4e2344311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca132eaef73648358d9939685e3f1307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de31fb9535b7432780405c6dff20351d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6df0fbcfa34818af05cecd40583f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/llama3-fine-tuning/tamil-llama3-8B-test-open-instruct-v1-peft/commit/48b914f564b2122ff7d896fcc50ab0626a23f1c9', commit_message='Upload tokenizer', commit_description='', oid='48b914f564b2122ff7d896fcc50ab0626a23f1c9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model = peft_model.merge_and_unload()\n",
    "\n",
    "model_save_name = \"llama3-fine-tuning/tamil-llama3-8B-test-open-instruct-v1-peft\"\n",
    "\n",
    "fine_tuned_model.push_to_hub(model_save_name)\n",
    "tokenizer.push_to_hub(model_save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8abb033-eeec-4c90-8b9e-f6198163c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush memory\n",
    "# del trainer, model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f06aae60-8072-4ecf-94a2-0c50fc2df210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /opt/app-root/src/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661e49a553344311ae83e08e09b2643e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98935f799c242c085f14d8e203dca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/89.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec00fa5f86c5428d9be746d8f433bfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33871352f804e769680c003f48b110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1078a22176a1426d89414bea9916ba41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b603bb6ee04c25b78774efde2ae719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8bdffc16f548b4a8d591c8be5da73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d6b3c2f82b4d2e8f94202c7d8276c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c772dd88134f13a47ee46b07c4acb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b3d2f2afc54ff6b47685b5b4fe21c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from transformers import pipeline \n",
    "\n",
    "from huggingface_hub import login\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    ")\n",
    "\n",
    "# CommitInfo(commit_url='https://huggingface.co/llama3-fine-tuning/tamil-llama3-8B-test-open-instruct-v1-peft/commit/48b914f564b2122ff7d896fcc50ab0626a23f1c9',\n",
    "#            commit_message='Upload tokenizer', commit_description='', oid='48b914f564b2122ff7d896fcc50ab0626a23f1c9', pr_url=None, pr_revision=None, pr_num=None)\n",
    "\n",
    "\n",
    "base_model =\"llama3-fine-tuning/tamil-llama3-8B-test-open-instruct-v1-peft\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "237827c5-1200-45e6-b38d-ca9aab455901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
      "\n",
      "### Instruction:\n",
      "நீங்கள் ஒரு வரலாற்றாசிரியர் என்று கற்பனை செய்து பாருங்கள். மறுமலர்ச்சிக் காலத்தில் அச்சகத்தின் முக்கியத்துவத்தையும் சமூகத்தில் அதன் தாக்கத்தையும் விளக்கவும்.\n",
      "\n",
      "### Response: \n",
      "நீங்கள் வரலாற்றாசிரியர் என்று கற்பன\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "prompt_template_wo_input=\"\"\"சரியான பதிலுடன் வேலையை வெற்றிகரமாக முடிக்க, தேவையான தகவலை உள்ளிடவும்.\n",
    "\n",
    "### Instruction:\n",
    "நீங்கள் ஒரு வரலாற்றாசிரியர் என்று கற்பனை செய்து பாருங்கள். மறுமலர்ச்சிக் காலத்தில் அச்சகத்தின் முக்கியத்துவத்தையும் சமூகத்தில் அதன் தாக்கத்தையும் விளக்கவும்.\n",
    "\n",
    "### Response:\"\"\"\n",
    "# device = \"auto\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# device = torch.device(\"cuda:3\")\n",
    "inputs = tokenizer(prompt_template_wo_input, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
