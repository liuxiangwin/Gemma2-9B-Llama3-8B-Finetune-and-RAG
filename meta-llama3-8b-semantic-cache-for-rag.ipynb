{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade  --quiet\\\n",
    "  \"transformers==4.38.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\" \\\n",
    "  \"langchain\" \\\n",
    "  \"sentence-transformers\" \\\n",
    "  \"faiss-cpu\"\n",
    "!pip install unstructured pdfminer pdfminer.six faiss-cpu \"torch==2.1.2\" tensorboard --quiet\n",
    "!pip install -U langchain-community==0.2.4  --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /opt/app-root/src/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ee0fac27284e4aa88169ae36142deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import display_markdown\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "import time\n",
    "from langchain.document_loaders import UnstructuredPDFLoader,PDFMinerLoader,TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    " \n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "terminators =  [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "### for semantic cache\n",
    "# vector_store = FAISS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Initialize an empty FAISS index\n",
    "dimension = embeddings.client.get_sentence_embedding_dimension()\n",
    "index = faiss.IndexFlatL2(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = InMemoryDocstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pdf file Path for RAG\n",
    "pdf_file_path = \"/opt/app-root/src/Gemma2-9B-Llama3-8B-Finetune-and-RAG/DeepLearningBook.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this class used to retrieve the text from pdf and chunk it \n",
    "class Langchain_RAG:\n",
    "    def __init__(self, pdf_file_path):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "        self.pdf_file_path = pdf_file_path\n",
    "        print(\"Loading PDF file, this may take time to process...\")\n",
    "        self.loader = PDFMinerLoader(self.pdf_file_path)\n",
    "        self.data = self.loader.load()\n",
    "        print(\"PDF file loaded.\")\n",
    "        print(\"Chunking...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "        self.texts = text_splitter.split_documents(self.data)\n",
    "        print(\"Chunking completed.\")\n",
    "        self.get_vec_value = FAISS.from_documents(self.texts, self.embeddings)\n",
    "        print(\"Vector values saved.\")\n",
    "        self.retriever = self.get_vec_value.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    def __call__(self, query):\n",
    "        relevant_docs = self.retriever.get_relevant_documents(query)\n",
    "        return \"\".join([doc.page_content for doc in relevant_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# This class is used to generate responses from an LLM model\n",
    "class Llama3_8B_gen:\n",
    "    def __init__(self, pipeline, embeddings, vector_store, threshold):\n",
    "        self.pipeline = pipeline\n",
    "        self.embeddings = embeddings\n",
    "        self.vector_store = vector_store\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_prompt(query,retrieved_text):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer the Question for the Given below context and information and not prior knowledge, only give the output result \\n\\ncontext:\\n\\n{}\".format(retrieved_text) },\n",
    "            {\"role\": \"user\", \"content\": query},]\n",
    "        return pipeline.tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)\n",
    "    \n",
    "    def semantic_cache(self, query, prompt):\n",
    "        query_embedding = self.embeddings.embed_documents([query])\n",
    "        similar_docs = self.vector_store.similarity_search_with_score_by_vector(query_embedding[0], k=1)\n",
    "        \n",
    "        if similar_docs and similar_docs[0][1] <self.threshold:\n",
    "            self.print_bold_underline(\"---->> From Cache\")\n",
    "            return similar_docs[0][0].metadata['response']\n",
    "        else:\n",
    "            self.print_bold_underline(\"---->> From LLM\")\n",
    "            output = self.pipeline(prompt, max_new_tokens=512, eos_token_id=terminators, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "            \n",
    "            response = output[0][\"generated_text\"][len(prompt):]\n",
    "            self.vector_store.add_texts(texts = [query], \n",
    "                       metadatas = [{'response': response},])\n",
    "            \n",
    "            return response\n",
    "            \n",
    "    def generate(self, query, retrieved_context):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        prompt = self.generate_prompt(query, retrieved_context)\n",
    "        res = self.semantic_cache(query, prompt)   \n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        self.print_bold_underline(f\"LLM generated in {execution_time:.6f} seconds\")\n",
    "        \n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def print_bold_underline(text):\n",
    "        print(f\"\\033[1m\\033[4m{text}\\033[0m\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF file, this may take time to process...\n",
      "PDF file loaded.\n",
      "Chunking...\n",
      "Chunking completed.\n",
      "Vector values saved.\n"
     ]
    }
   ],
   "source": [
    "text_gen = Llama3_8B_gen(pipeline=pipeline,embeddings=embeddings,\n",
    "                         vector_store=vector_store,threshold=0.1)\n",
    "retriever = Langchain_RAG(pdf_file_path=pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rag_qa(query):\n",
    "    retriever_context = retriever(query)\n",
    "    result = text_gen.generate(query,retriever_context)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4m---->> From LLM\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4mLLM generated in 5.835521 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the given context, Deep Learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain, statistics and applied math. It is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_qa(\"What is Deep learning ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4m---->> From Cache\u001b[0m\n",
      "\u001b[1m\u001b[4mLLM generated in 0.009813 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the given context, Deep Learning is an approach to machine learning that has drawn heavily on our knowledge of the human brain, statistics and applied math. It is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_qa(\"What is Deep learning ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4m---->> From LLM\u001b[0m\n",
      "\u001b[1m\u001b[4mLLM generated in 21.266562 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The back-propagation algorithm is a method for efficiently computing the gradient of the loss function with respect to the model\\'s parameters, which is an essential step in training neural networks. The algorithm is called \"back-propagation\" because it propagates the error backwards through the network, from the output layer to the input layer.\\n\\nHere\\'s a step-by-step explanation of the back-propagation algorithm:\\n\\n1. **Forward Pass**: Start by performing a forward pass through the network, computing the output for a given input and storing the intermediate results.\\n2. **Error Calculation**: Calculate the error between the predicted output and the actual output.\\n3. **Backward Pass**: Traverse the network in reverse order, from the output layer to the input layer, and compute the gradients of the loss function with respect to the model\\'s parameters.\\n4. **Gradient Computation**: At each layer, compute the gradient of the loss function with respect to the layer\\'s output, using the error from the previous step.\\n5. **Parameter Update**: Use the computed gradients to update the model\\'s parameters using an optimization algorithm, such as stochastic gradient descent.\\n\\nThe key insight behind back-propagation is that the gradients of the loss function with respect to the model\\'s parameters can be computed efficiently by propagating the error backwards through the network. This allows us to compute the gradients of the loss function with respect to the model\\'s parameters, which is necessary for training the model.\\n\\nIn the given context, the back-propagation algorithm is applied to the unrolled graph associated with a fully-connected multi-layer MLP. The algorithm is shown in Algorithm 6.6, which computes the gradient of the loss function with respect to the model\\'s parameters.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_qa(\"Explain back propagation algorithm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4m---->> From LLM\u001b[0m\n",
      "\u001b[1m\u001b[4mLLM generated in 5.019125 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The back-propagation algorithm involves performing a forward propagation pass moving left to right through the graph, followed by a backward propagation pass moving right to left through the graph. The runtime is O(τ) and cannot be reduced by parallelization because the forward propagation graph is inherently sequential; each time step may only be computed after the previous one.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_qa(\"back propagation algorithm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[4m---->> From Cache\u001b[0m\n",
      "\u001b[1m\u001b[4mLLM generated in 0.009828 seconds\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The back-propagation algorithm involves performing a forward propagation pass moving left to right through the graph, followed by a backward propagation pass moving right to left through the graph. The runtime is O(τ) and cannot be reduced by parallelization because the forward propagation graph is inherently sequential; each time step may only be computed after the previous one.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rag_qa(\"back propagation algorithm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "When generating text directly from the Large Language Model (LLM), the process may take over 40 seconds. However, by caching the generated text, subsequent requests for the same text experience significantly reduced response times. This caching mechanism stores previously generated text, allowing for quick retrieval without the need to regenerate it, thus improving response times for repetitive requests. By leveraging this cache, the system optimizes performance and enhances user experience by minimizing wait times for text generation."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4828888,
     "sourceId": 8161765,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
